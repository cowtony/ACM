二分法(binary search)是面试中常见的算法。如果你的学校有算法课，那么你可能已经学习过二分这个算法了。

但是很多同学在二分法的问题上，依然无法熟练掌握，以下问题经常会不断发生：
- 写出来的二分法总是死循环
- start + 1 < end 还是 start <= end 还是 start < end 总是搞不清楚
- start = mid + 1 还是 start = mid 也总是搞不清楚

在这一章的学习中，我们要会学到：
- 一个通用的二分模板
- 为什么会出现死循环
- 哪些题目可以用二分法来做
- 通过时间复杂度倒推算法的技巧
- 其他的 Log N 算法（倍增法，辗转相除法，快速幂算法）

先修内容有：
- 基本的二分法该如何写
- 第一个位置，最后一个位置该如何变化
- Big O，时间复杂度，空间复杂度
- 什么是递归，二分法用递归如何实现
- 内存中的栈空间和堆空间有什么区别
- 什么是 Stack Overflow，什么情况下会造成 Stack Overflow

补充内容有：
- 三步翻转法
- 递增矩阵找数
- 快速幂算法
- 辗转相除法

[LintCode 上的二分法题目](http://www.lintcode.com/tag/binary-search)

可以看到，二分的题目非常的多

# 什么是算法的时间复杂度
我们用复杂度来量化一个算法的时间，空间。在这一小节中，我们讲学习什么是复杂度，什么是时间复杂度，什么是空间复杂度。

在面试中，时间复杂度是问得比较多的，空间复杂度一般不会问。

时间复杂度是面试中必问的问题。学好时间复杂度，有如下的帮助：
- 面试官会问你的算法时间复杂度是什么
- 当面试官说，有没有更好的方法时，你知道朝什么样的复杂度优化
- 利用时间复杂度倒推算法是面试常用技巧。如 O(logN)的算法几乎可以确定是二分法。

一个算法的运行时间与其所要执行的语句的数量成正比，而所要执行的语句与问题规模正相关。因此算法的时间复杂度可以表示为一个与问题规模 N 相关的多项式。

例如下面的代码：
Java版本：
```
int sum = 0;
int n = 100;
for (int i = 0; i < n; i++) {
   sum += i;
}
for (int i = 0; i < n; i++) {
   sum += i;
}
```
Python版本：
```
sum = 0
n = 100
for i in range(n):
    sum += i
for i in range(n):
    sum += i
```
这段代码的运行时间与n的大小正相关，因此，我们可以将其复杂度写成多项式：f(n)=2n;

在计算机科学中，时间复杂度使用标记O(大写英文字母o)表示，只包含上述多项式中的最高次项，且忽略最高次项的系数

上面这段代码的时间复杂度就是：O(n)。

时间复杂度定性的描述了一个算法的运行时间。（定义参照[WIKI](https://zh.wikipedia.org/wiki/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6)）

更为通俗的定义是：程序执行了多少句语句，一条一句被多次执行，就要算多次。

时间复杂度计算的要点：
- 只包含多项式的最高次项。 这是因为在复杂度计算中，最高次项对运行时间有决定性的作用，次高次项可以忽略不计。例如f(n) = n^2 + n， 此时n这一项对于多项式的值的影响想对于n^2 可以忽略不计。在定性的描述中，我们只取最高次项。
- 不包含多项式最高次项的系数。 对于最高次项，我们忽略它的系数，在算法中，我们称之为常数。上面代码中，常数是2,但是时间复杂度的计算，我们只取O(n)。

在计算算法的时间复杂度时，我们关注其中最耗时的部分，对于相对不那么耗时的部分就忽略不去考虑。也就是上面讲的”只保留最高次项“。

例子：

Java:
```
int sum = 0;
int n = 100;
for (int i = 0; i < n; i++) {
   for (int j = 0; j < n; j++) {
      sum++;
   }
}
for (int i = 0; i < n; i++) {
    sum++;
}
```
Python:
```
sum = 0
n = 100
for i in range(n):
    for j in range(n):
        sum += 1
for i in range(n):
    sum += 1
```
算法中，常见的时间复杂度有：

|复杂度|可能对应的语法|备注|
|-------|----------------|-----|
|O(1)|位运算|常数级复杂度，一般面试中不会有
|O(logn)|二分法，倍增法，快速幂算法，辗转相除法||
|O(n)|枚举法，双指针算法，单调栈算法，KMP算法，Rabin Karp，Manacher's Algorithm	又称作线性时间复杂度||
|O(nlogn)|快速排序，归并排序，堆排序||
|O(n^2)|枚举法，动态规划，Dijkstra||
|O(n^3)|枚举法，动态规划，Floyd||
|O(2^n)|与组合有关的搜索问题||
|O(n!)|与排列有关的搜索问题||

在面试中，经常会涉及到时间复杂度的计算。当你在对于一个问题给出一种解法之后，面试官常会进一步询问，是否有更优的方法。此时就是在问你是否有时间复杂度更小的方法（有的时候也要考虑空间复杂度更小的方法），这个时候需要你对常用的数据结构操作和算法的时间复杂度有清晰的认识，从而分析出可优化的部分，给出更优的算法。

例如，给定一个已经排序的数组，现在有多次询问，每次询问一个数字是否在这个数组中，返回True or False.
- 方法1： 每次扫描一遍数组，查看是否存在。
	这个方法，每次查询的时间复杂度是: O(n)。
- 方法2：由于已经有序，可以使用二分查找的方法。
	这个方法，每次查询的时间复杂度是: O(logn)。
- 方法3：将数组中的数存入Hashset。
	这个方法，每次查询的时间复杂度是: O(1)。

可以看到，上述的三种方法是递进的，时间复杂度越来越小。

在面试中还有很多常见常用的方法，他们的时间复杂度并不是固定的，都需要掌握其时间复杂度的分析，要能够根据算法过程自己推算出时间复杂度。

## 用 T 函数表示法计算时间复杂度
接下来我们来接受一种计算时间复杂度的方法 --- 用 T 函数表示法计算时间复杂度。

### T 函数推导法
我们介绍一种时间复杂度的推导方法：T函数推导法

比如二分法。二分法是每次通过 O(1) 的时间将规模为 n 的问题降低为规模为 n/2的问题。

这里我们用 T(n) 来表示规模为 n 的问题在该算法下的时间复杂度，那么我们得出推导公式：
```
T(n) = T(n/2) + O(1)
```
我们来逐个说明一下这个公式的意义。

首先 T 代表的是 Time Complexity,n 代表的是问题规模（二分法里就是数组的大小）。

那么 T(n) 代表的就是：求处理问题规模为n的数据的时间复杂度是多少。注意这里是一个问句，不是一个答案。

T(n) 根据算法的不同可以是O(n), 也可以是 O(nlogn)或任何值，而 O(n) 就是 O(n)。

然后 O 代表的是时间复杂度。O(1) 就意味着，你大概用一个 if 语句，或者简单的加加减减，就可以完成。O 在这里的意思是数量级约等于。在 O 的世界里，我们只考虑最高项是什么，不考虑系数和常数项。比如：
- `O(100n) = O(n)`
- `O(n^2 + n) = O(n^2)`
- `O(2^n + n^2 + 10) = O(2^n)`

### 如何推导 T 函数
我们可以使用不断展开的方法进行推导：
```
T(n) = T(n/2) + O(1)
     = T(n/4) + O(1) + O(1)
     = T(n/8) + O(1) * 3
     = T(n/16) + O(1) * 4
     ...
     = T(1) + O(1) * logn
     = O(logn)
```
在时间复杂度的领域里，有如下的一些性质：
- T(1) = O(1)// 解决规模为1的问题通常时间复杂度为O(1)。这个不100%对，但是99.9%的情况下都是如此。
- k * O(n) = O(kn)
- O(n) + O(m) = O(n + m)

上面的方法，是采用 T 函数展开的方法，将二分法的时间复杂度最终用 O(...) 来表示

# 空间复杂度
接下来我们来讲一讲复杂度理论的另一部分，空间复杂度。

面试中很少问到空间复杂度，因为一般来说你看一下你开了多大的数组就是多少，比较简单。

本小节中我们快速的学习一下空间复杂度的相关知识。

类似于时间复杂度，空间复杂度就是衡量算法运行时所占用的临时存储空间的度量。也是一个与问题规模n有关的函数。

我们同样使用O(大写字母o)来标记。

算法所占用的空间主要有三个方面：算法代码本身占用的空间、输入输出数据占用的空间、算法运行时临时占用的空间。

其中，代码本身和输入输出数据占用的空间不是算法空间复杂度考虑的范围内，空间复杂度只考虑运行时临时占用的空间，又称为算法的额外空间（Extra space）。

临时占用的空间包括：
- 为参数列表中形参变量分配的空间
- 为函数体中局部变量分配的空间

（如果是递归函数，需要将上述两部分占用空间的和乘以递归的深度，这是堆栈空间，在下面小节中详细讲解这部分）

例如：

Java:
```
public void insertionSort(int[] A) {
    int n = A.length; 
    for(int i = 1; i < n; i++){
       int t = A[i];
       int index = 0;
       for (int j = i - 1; j >= 0; j--){
           if (A[j] > t){
              A[j + 1] = A[j];
           } else {
              index = j + 1;
              break;
           }
       }
       A[index] = t;
    }
}
```
Python:
```
def insertionSort(A):
    n = len(A)
    for i in range(1, n):
        t = A[i]
        index = 0
        j = i - 1
        while j >= 0:
            if A[j] > t:
                A[j + 1] = A[j]
            else:
                index = j + 1
                break
            j -= 1
        A[index] = t
    return A
```
C++:
```
void insertionSort(vector<int> A) { 
    int n = A.size(); 
    for(int i = 1; i < n; i++){
       int t = A[i];
       int index = 0;
       for (int j = i - 1; j >= 0; j--){
           if (A[j] > t){
              A[j + 1] = A[j];
           } else {
              index = j + 1;
              break;
           }
       }
       A[index] = t;
    }
}
```
上面这段代码是插入排序的一种实现，其中主要占用的临时空间包括：变量n、t、index、i、j。int[] A是我们的输入数据，不在我们的临时空间范围内。

因此这段代码的空间复杂性函数可以写成：f(n) = 5f(n)=5，因为无论数组多大，变量的数量就是这么多。

这段代码的额外空间复杂度是: O(1)。

与时间复杂度一样，我们只计算最高次项，且忽略最高次项的系数。此处最高次项是 n^0 = 1。

在面试中，很多时候面试官给出的问题会附带一个“不能使用多余的空间”这样的要求。很多时候这是在要求你的空间复杂度只能是O(1)的，也就是你只能开几个辅助变量，而不能开大数组。

其他常见的还有，分析一下你的算法空间复杂度，寻找空间复杂度更优的解法等。

一般来说，算法占用的时间和空间会是两个互相平衡的元素，有的时候我们牺牲空间来换取时间，有的时候我们牺牲时间来换取空间。

在面试中常见算法的空间复杂度：
- 快速排序： 最优：O(logn)，最差:O(n)
- 二分查找： O(1)
- 最短路(Dijkstra)算法： O(V)(V表示点集大小)

## 运行时堆栈
在递归函数中，除了变量和数组所开辟的临时空间以外，还有一个空间我们需要纳入考虑，就是递归时占用的栈空间（Stack）。

递归函数需要保存当前的环境，以便在递归返回的时候能够还原之前的现场。因此递归的深度越深，所要占用的栈空间越大。当空间超出一定范围的时候就会出现程序爆栈（Stack Overflow）的情况。

很多博客文章中会写堆栈空间与递归调用的次数成正比，这个是不完全正确的，应该是与递归的深度成正比（此处只讨论单线程）。

因为递归在返回到上一层的时侯，就会将本层的空间释放，因此占用的栈空间不会比最深的一次调用所占用的空间更多。

## 如何分析空间复杂度
### 大部分的空间复杂度计算方法
累加下面两个部分的内容即是你代码的空间复杂度：
- 你的代码里开辟了多少新的空间（new 了多少新的内容出来）
- 你的递归深度 * 递归函数内部的参数和局部变量所占用的空间

#### 以快速排序为例子
快速排序的思路如下：
- 选择一个基准元素，将原数组分为两部分，左边部分小于该元素，右边部分大于该元素。
- 分别递归处理左边和右边。

最好情况：每次都能恰好将数组分成左右相同长度的两部分，需要的递归深度是:lgn，每次将数组分成两部分时，我们选择不使用辅助数组，在原数组上“就地”处理，所以每层的空间是O(1)。因此总的复杂度是:O(logn)

最差情况： 每次都将数组分成长度差最大的两部分，即一边只有一个元素，其余的在另外一边，最大深度为：n， 因此空间复杂度为:O(n)。

有些递归算法的空间复杂度是稳定的，不会退化，快排的递归深度与其每次选择的“基准值”有很大关系，因此存在退化的情况。

接下来，我们来看一个不常见的时间复杂度的例子，sqrt{n}的时间复杂度。

#### 分解质因数
以 sqrt{n} 为时间复杂度的算法并不多见，最具代表性的就是分解质因数了。

具体步骤
- 记up = sqrt{n}，作为质因数k的上界, 初始化k=2。
- 当k <= up 且 n不为1 时，执行步骤3，否则执行步骤4。
- 当n被k整除时，不断整除并覆盖n，同时结果中记录k，直到n不能整出k为止。之后k自增，执行步骤2。
- 当n不为1时，把n也加入结果当中，算法结束。

几点解释
- 不需要判定k是否为质数，如果k不为质数，且能整出n时，n早被k的因数所除。故能整除n的k必是质数。
- 为何引入up？为了优化性能。当k大于up时，k已不可能整除n，除非k是n自身。也即为何步骤4判断n是否为1，n不为1时必是比up大的质数。
- 步骤2中，也判定n是否为1，这也是为了性能，当n已为1时，可早停。

代码

Java:
```
public List<Integer> primeFactorization(int n) {
    List<Integer> result = new ArrayList<>();
    int up = (int) Math.sqrt(n);
    
    for (int k = 2; k <= up && n > 1; ++k) {
        while (n % k == 0) {
            n /= k;
            result.add(k);
        }
    }
    
    if (n > 1) {
        result.add(n);
    }
    
    return result;
}
```
Python:
```
def primeFactorization(n):
    result = []
    up = int(math.sqrt(n));
    
    k = 2
    while k <= up and n > 1: 
        while n % k == 0:
            n //= k
            result.append(k)
        k += 1
            
    if n > 1:
        result.append(n)

    return result
```
C++:
```
vector<int> primeFactorization(int n) {
    vector<int> result;
    int up = (int)sqrt(n);
    
    for (int k = 2; k <= up && n > 1; ++k) {
        while (n % k == 0) {
            n /= k;
            result.push_back(k);
        }
    }
    
    if (n > 1) {
        result.push_back(n);
    }
    
    return result;
}
```
复杂度分析
- 最坏时间复杂度O(sqrt (n) )。当n为质数时，取到其最坏时间复杂度。
- 空间复杂度O(log(n)), 当n质因数很多时，需要空间大，但总不会多于O(log(n))个

延伸

质因数分解有一种更快的算法，叫做Pollard Rho快速因数分解。该算法时间复杂度为O(n^{1/4})，其理解起来稍有难度，有兴趣的同学可以进行自学，[参考链接](https://wenku.baidu.com/view/3db5c7a6ad51f01dc381f156.html)。

# 二分法


接下来，我们就可以正式开始学习二分法了。

（Video Here）

在这接下来的一小节中，我们要学习什么是递归，以及如何用递归的方式来实现二分法。

首先我们举个例子来理解递归——用递归实现斐波那契数列

（Video Here）

（Video Here）

# 内存中的栈空间与堆空间
为了分析这种写法的空间复杂度，我们需要了解一下内存中，栈和堆占用了多少空间

我们通常所说的内存空间，包含了两个部分：栈空间（Stack space）和堆空间（Heap space）

当一个程序在执行的时候，操作系统为了让进程可以使用一些固定的不被其他进程侵占的空间用于进行函数调用，递归等操作，会开辟一个固定大小的空间（比如 8M）给一个进程使用。这个空间不会太大，否则内存的利用率就很低。这个空间就是我们说的栈空间，Stack space。

我们通常所说的栈溢出（Stack Overflow）是指在函数调用，或者递归调用的时候，开辟了过多的内存，超过了操作系统余留的那个很小的固定空间导致的。那么哪些部分的空间会被纳入栈空间呢？栈空间主要包含如下几个部分：
- 函数的参数与返回值
- 函数的局部变量

我们来看下面的这段代码：

Java:
```
public int f(int n) {
    int[] nums = new int[n];
    int sum = 0;
    for (int i = 0; i < n; i++) {
        nums[i] = i;
        sum += i;
    }
    return sum;
}
```
Python:
```
def f(n):
    nums = [0]*n  # 相当于Java中的new int[n]
    sum = 0
    for i in range(n):
        nums[i] = i
        sum += i
    return sum
```
C++:
```
int f(int n) {
   int *nums = new int[n];
    int sum = 0;
    for (int i = 0; i < n; i++) {
        nums[i] = i;
        sum += i;
    }
    return sum;
}
```
根据我们的定义，参数 n，最后的函数返回值f，局部变量 sum 都很容易的可以确认是放在栈空间里的。那么主要的难点在 nums。

这里 nums 可以理解为两个部分：
- 一个名字叫做 nums 的局部变量，他存储了指向内存空间的一个地址（Reference），这个地址也就是 4 个字节（32位地址总线的计算机，地址大小为 4 字节）
- new 出来的，一共有 n 个位置的整数数组，int[n]。一共有 4 * n 个字节。

这里 nums 这个变量本身，是存储在栈空间的，因为他是一个局部变量。但是 nums 里存储的 n 个整数，是存储在**堆空间**里的，Heap space。他并不占用栈空间，并不会导致栈溢出。

在大多数的编程语言中，特别是 Java, Python 这样的语言中，万物皆对象，基本上每个变量都包含了变量自己和变量所指向的内存空间两个部分的逻辑含义。

来看这个例子：

Java:
```
public int[] copy(int[] nums) {
    int[] arr = new int[nums.length];
    for (int i = 0; i < nums.length; i++) {
        arr[i] = nums[i]
    }
    return arr;
}

public void main() {
    int[] nums = new int[10];
    nums[0] = 1;
    int[] new_nums = copy(nums);
}
```
Python:
```
def copy(nums):
    arr = [0]*len(nums)  # 相当于Java中的new int[nums.length]
		for i in range(len(nums)):
        arr[i] = nums[i]
    return arr
		
# 用list comprehension实现同样功能
def copy(nums):
    arr = [x for x in nums]
    return arr
		
# 以下相当于Java中的main函数
if __name__ == "__main__":
    nums = [0]*10
    nums[0] = 1
    new_nums = copy(nums)
```
C++
```
int* copy(int nums[], int length) {
    int *arr = new int[length];
    for (int i = 0; i < length; i++) {
        arr[i] = nums[i];
    }
    return arr;
}

int main() {
    int *nums = new int[10];
    nums[0] = 1;
    int *new_nums = copy(nums, 10);
	return 0;
}
```
在 copy 这个函数中，arr 是一个局部变量，他在 copy 函数执行结束之后就会被销毁。但是里面 new 出来的新数组并不会被销毁。

这样，在 main 函数里，new_nums 里才会有被复制后的数组。所以可以发现一个特点：
> 栈空间里存储的内容，会在函数执行结束的时候被撤回

简而言之可以这么区别栈空间和堆空间：
> new 出来的就放在堆空间，其他都是栈空间

# 什么是递归深度
递归深度就是递归函数在内存中，同时存在的最大次数。

例如下面这段求阶乘的代码：

Java:
```
int factorial(int n) {
    if (n == 1) {
        return 1;
    }
    return factorial(n - 1) * n;
}
```
Python:
```
def factorial(n):
    if n == 1:
        return 1
    return factorial(n-1) * n
```
C++:
```
int factorial(int n) {
    if (n == 1) {
        return 1;
    }
    return factorial(n - 1) * n;
}
```
当`n=100`时，递归深度就是100。一般来说，我们更关心**递归深度的数量级**，在该阶乘函数中递归深度是O(n)，而在二分查找中，递归深度是O(log(n))。在后面的教程中，我们还会学到基于递归的快速排序、归并排序、以及平衡二叉树的遍历，这些的递归深度都是(O(log(n))。注意，此处说的是递归深度，而并非时间复杂度。

## 太深的递归会内存溢出
首先，函数本身也是在内存中占空间的，主要用于存储传递的参数，以及调用代码的返回地址。

函数的调用，会在内存的栈空间中开辟新空间，来存放子函数。递归函数更是会不断占用栈空间，例如该阶乘函数，展开到最后n=1时，内存中会存在factorial(100), factorial(99), factorial(98) ... factorial(1)这些函数，它们从栈底向栈顶方向不断扩展。

当递归过深时，栈空间会被耗尽，这时就无法开辟新的函数，会报出stack overflow这样的错误。
所以，在考虑空间复杂度时，递归函数的深度也是要考虑进去的。

**Follow up：**

尾递归：若递归函数中，递归调用是整个函数体中最后的语句，且它的返回值不属于表达式的一部分时，这个递归调用就是尾递归。（上例factorial函数满足前者，但不满足后者，故不是尾递归函数）

尾递归函数的特点是：在递归展开后该函数不再做任何操作，这意味着该函数可以不等子函数执行完，自己直接销毁，这样就不再占用内存。一个递归深度O(n)的尾递归函数，可以做到只占用O(1)空间。这极大的优化了栈空间的利用。

但要注意，这种内存优化是由编译器决定是否要采取的，不过大多数现代的编译器会利用这种特点自动生成优化的代码。在实际工作当中，尽量写尾递归函数，是很好的习惯。

而在算法题当中，计算空间复杂度时，建议还是老老实实地算空间复杂度了，尾递归这种优化提一下也是可以，但别太在意。

# 模版代码剖析
接下来，我们来看一下，当遇到需要用到二分法来解决问题时，是否有一种通用的模版。

(Video Here)

Java 版本
```
public class Solution {
    /**
     * @param A an integer array sorted in ascending order
     * @param target an integer
     * @return an integer
     */
    public int findPosition(int[] nums, int target) {
        if (nums == null || nums.length == 0) {
            return -1;
        }
        
        int start = 0, end = nums.length - 1;
        // 要点1: start + 1 < end
        while (start + 1 < end) {
	    // 要点2：start + (end - start) / 2
            int mid = start + (end - start) / 2;
            // 要点3：=, <, > 分开讨论，mid 不+1也不-1
            if (nums[mid] == target) {
                return mid;
            } else if (nums[mid] < target) {
                start = mid;
            } else {
                end = mid;
            }
        }
        
        // 要点4: 循环结束后，单独处理start和end
        if (nums[start] == target) {
            return start;
        }
        if (nums[end] == target) {
            return end;
        }
        return -1;
    }
}
```
其他语言的参考代码请见：
http://www.jiuzhang.com/solutions/binary-search/

常见问题

Q: 为什么要用 start + 1 < end？而不是 start < end 或者 start <= end？
>A: 为了避免死循环。二分法的模板中，整个程序架构分为两个部分：
通过 while 循环，将区间范围从 n 缩小到 2 （只有 start 和 end 两个点）。
在 start 和 end 中判断是否有解。
start < end 或者 start <= end 在寻找目标最后一次出现的位置的时候，出现死循环。

Q: 为什么明明可以 start = mid + 1 偏偏要写成 start = mid?
>A: 大部分时候，mid 是可以 +1 和 -1 的。在一些特殊情况下，比如寻找目标的最后一次出现的位置时，当 target 与 nums[mid] 相等的时候，是不能够使用 mid + 1 或者 mid - 1 的。因为会导致漏掉解。那么为了节省脑力，统一写成 start = mid / end = mid 并不会造成任何解的丢失，并且也不会损失效率——log(n) 和 log(n+1) 没有区别。

https://www.lintcode.com/problem/last-position-of-target/

**为什么 start < end 容易出现死循环**
许多同学在写二分法的时候，都比较习惯性的写while (start < end)这样的循环条件。这样的写法及其容易出现死循环，导致 LintCode 上的测试“超时”（Time Limit Exceeded）。

什么情况下会出现死循环？

在做 last position of target这种模型下的二分法时，使用 while (start < end) 就容易出现超时。

我们来看看会超时的代码：

Java版本：
```
int start = 0, end = nums.length - 1;
while (start < end) {
    int mid = start + (end - start) / 2;
    if (nums[mid] == target) {
        start = mid;
    } else if (nums[mid] < target) {
        start = mid + 1;
    } else {
        end = mid - 1;
    }
}
```
Python:
```
start, end = 0, len(nums) - 1
while start <end:
    mid = start + (end - start) // 2
    if nums[mid] == target:
        start = mid
    else if nums[mid] < target:
        start = mid + 1
    else:
        end = mid - 1
```
C++:
```
int start = 0, end = nums.size() - 1; // nums 是 vector<int>
while (start < end) {
    int mid = start + (end - start) / 2;
    if (nums[mid] == target) {
        start = mid;
    } else if (nums[mid] < target) {
        start = mid + 1;
    } else {
        end = mid - 1;
    }
}
```
假如，nums = [1,1], target = 1

将数据带入过一下代码：
```
start = 0, end = 1
while (0 < 1) {
    mid = 0 + (1 - 0) / 2 = 0
    if (nums[0] == 1) {
        start = 0;
      }
    ...
}
```
我们发现，start 将始终是 0。

出现这个问题的主要原因是，mid = start + (end - start) / 2这种写法是偏向start的。也就是说 mid 是中间偏左的位置。这样导致如果 start 和 end 已经是相邻关系，会导致 start 有可能在一轮循环之后保持不变。

或许你会说，那么我改成 mid = (start + end + 1) / 2是否能解决问题呢？没错，确实可以解决 last position of target 的问题，但是这样之后 first position of target 就超时了。我们比较希望能够有一个理想的模板，无论是 first position of target 还是 last position of target，代码的区别尽可能的小和容易记住。

以上就是第三章所需要的所有知识，学有余力的同学可以继续学习我们的课后补充内容，包括三步翻转法，二维矩阵找数问题，辗转相除法，快速幂算法和两个排序数组的中位数问题。

# 三步翻转法
Video: 3-step-reverse

# 二维矩阵找数问题
Video: search a 2d matrix

# [辗转相除法](https://www.lintcode.com/problem/greatest-common-divisor/)
## 算法介绍
辗转相除法， 又名欧几里德算法， 是求最大公约数的一种方法。它的具体做法是：用较大的数除以较小的数，再用除数除以出现的余数（第一余数），再用第一余数除以出现的余数（第二余数），如此反复，直到最后余数是0为止。如果是求两个数的最大公约数，那么最后的除数就是这两个数的最大公约数。

代码

Java:
```
public int gcd(int big, int small) {
    if (small != 0) {
        return gcd(small, big % small);
    } else {
        return big;
    }
}
```
Python:
```
def gcd(big, small):
    if small != 0:
        return gcd(small, big % small)
    else:
        return big
```
C++:
```
int gcd(int big, int small) {
    if (small != 0) {
        return gcd(small, big % small);
    } else {
        return big;
    }
}
```

# [矩阵快速幂](https://www.lintcode.com/problem/fast-power/description)
## 基本原理
计算x的n次方， 即计算x^n。

由公式可知： x^n = x^{n/2} * x^{n/2}。

如果我们求得x^{n/2}， 则可以O(1)求出x^n, 而不需要再去循环剩下的n/2次。

以此类推，若求得x^{n/4}，则可以O(1)求出x^{n/2} 。

。。。

因此一个原本O(n)的问题，我们可以用O(logn)复杂度的算法来解决。

递归版本的快速幂算法

Java:
```
int power(int x, int n) {
    if (n == 0) return 1;
    if (n % 2 == 0) {
        int tmp = power(x, n / 2);
        return tmp * tmp;
    } else {
        int tmp = power(x, n / 2);
        return tmp * tmp * x;
    }
}
```
Python:
```
def power(x, n):
    if n == 0:
        return 1
    
    if n % 2 == 0:
        tmp = power(x, n // 2)
        return tmp * tmp
    else:
        tmp = power(x, n // 2)
        return tmp * tmp * x
```
C++:
```
int power(int x, int n) {
    if (n == 0) return 1;
    if (n % 2 == 0) {
        int tmp = power(x, n / 2);
        return tmp * tmp;
    } else {
        int tmp = power(x, n / 2);
        return tmp * tmp * x;
    }
}
```
注意：
- 不要重复计算，在计算x^{n/2} * x^{n/2}的时候，先计算出x^{n/2} ，存下来然后返回tmp*tmp;
- n为奇数的时候要记得再乘上一个xx。

非递归版本

Java:
```
int power(int x, int n) {
    int ans = 1, base = x;
    while (n != 0) {
        if (n % 2 == 1) {
            ans *= base;
        }
        base *= base;
        n = n / 2;
    }
    return ans;
}
```
Python:
```
def power(x, n):
    ans = 1
    base = x
    while n > 0:
        if n % 2 == 1:
            ans *= base
        base *= base
        n = n // 2
    return ans
```
C++:
```
int power(int x, int n) {
    int ans = 1, base = x;
    while (n != 0) {
        if (n % 2 == 1) {
            ans *= base;
        }
        base *= base;
        n = n / 2;
    }
    return ans;
}
```
非递归版本与递归版本原理相同，计算顺序略有不同。

因为递归是从大问题进入，划分子问题然后层层返回再求解大问题。这里要从小问题开始，直接求解大问题。

你可以打印出每次循环中 base 和 ans 的值，来理清楚其中的算法思路。

递归版本和非递归版本都应该熟练掌握，虽然递归版本更容易掌握和理解，且 logn 的计算深度也不会导致 Stack Overflow。但是面试官是很有可能为了加大难度让你在用非递归的版本写一遍的。




# [两个排序数组的中位数](https://www.lintcode.com/problem/median-of-two-sorted-arrays/)
接下来我们来看一下求两个排序数组的中位数这道题，

题目描述

在两个排序数组中，求他们合并到一起之后的中位数

时间复杂度要求：O(log(n+m))，其中 n, m 分别为两个数组的长度

解法
- 基于 FindKth 的算法。整体思想类似于 median of unsorted array 可以用 find kth from unsorted array 的解题思路。
- 基于二分的方法。二分 median 的值，然后再用二分法看一下两个数组里有多少个数小于这个二分出来的值。

## 基于 FindKth 的算法
算法描述
- 先将找中点问题转换为找第 k 小的问题，这里直接令k = (n + m) / 2。那么目标是在 logk = log((n+m)/2) = log(n+m) 的时间内找到A和B数组中从小到大第 k 个。
- 比较 A 数组的第 k/2 小和 B 数组的第 k/2 小的数。谁小，就扔掉谁的前 k/2 个数。
- 将目标寻找第 k 小修改为寻找第 (k-k/2) 小
- 回到第 2 步继续做，直到 k == 1 或者 A 数组 B 数组里已经没有数了。

F.A.Q

Q: 如何 O(1) 时间移除数组的前 k/2 个数？

A: 给两个数组一个起始位置的标记参数（相当于一个offset，偏移位），把这个起始位置 + k/2 就可以了。

Q: 不是让我们找中点么？怎么变成了找第 k 小？

A: 找第 k 小如果能在 log(k) 的时间内解决，那么找中点就可以在 log( (n+m)/2 ) 的时间内解决。

Q.如何证明谁的第 k/2 个数比较小就扔掉谁的前 k/2 个数这个理论？

A: 直观的，我们看一个例子
```
A=[1,3,5,7]
B=[2,4,6,8]
```
假如我们要找第 4 小。也就是 k = 4。算法会去比较两个数组中第 2 小的数。也就是 A[1]=3 和 B[1]=4 这两个数的大小。然后会发现，3比较小，然后就决定扔掉 A 的前 k/2 = 2 个数。也就是，接下来，需要去找
```
A=[5,7]
B=[2,4,6,8]
```
中的第 k-k/2=2 小的数。这里我们扔掉了 [1,3]，扔掉的这些数中，一定不会包含我们要找的第 4 小的数——4。因为从位置上，他们在 A 和 B合并到一起之后，都会排在 4 的前面。

抽象的证明一下：

我们需要回顾一下 Merge Two Sorted Arrays 这道题目。算法的做法是，每一次比较两个数组中比较小的数，然后谁小，谁先被拿出来，放到最后的合并结果中。那么假设 A 和 B中 A[k/2 - 1] <= B[k/2 - 1]（反之同理）。我们会决定扔掉A[0..k/2-1]，因为这些数在 A 与 B 做简单的 Merge 的过程中，会优先于目标第 k 个数现出来。为什么？因为既然A[k/2-1] <= B[k/2-1]，那么当我们用最简单的 Merge Two Sorted Arrays 的算法一个个从A和B里拿数出来的时候，当 A[k/2 - 1] 出来的时候，B[k/2 - 1] 一定还没有被拿出来，那么此时A里出来了 k/2 个数，B里出来的数一定不够 k/2 个（因为第 k/2 个数都还没出来），所以加起来总共出来的数肯定不够k个，所以第k小的数一定还留在AB数组中。

因此我们证明了：扔掉较小的一部分的前 k/2 个数，不会扔掉要找的第 k 小的数。

## 基于二分的算法
算法描述
- 我们需要先确定二分的上下界限，由于两个数组 A, B 均有序，所以下界为 min(A[0], B[0])，上界为 max(A[A.length - 1], B[B.length - 1]).
- 判断当前上下界限下的 mid(mid = (start + end) / 2) 是否为我们需要的答案；这里我们可以分别对两个数组进行二分来找到两个数组中小于等于当前 mid 的数的个数cnt1与 cnt2，sum = cnt1 + cnt2 即为 A 跟 B 合并后小于等于当前mid的数的个数.
- 如果sum < k，即中位数肯定不是mid，应该大于 mid，更新 start 为mid，否则更新 end 为mid，之后再重复第二步
- 当不满足 start + 1 < end 这个条件退出二分循环时，再分别判断一下start跟 end ，最终返回符合要求的那个数即可

算法详解

如果对该算法有点疑问，我们下面来详细讲解一下：
- 这一题如果用二分法来做，其实就是一个二分答案的过程
- 首先我们已经得到了上下界限，那么答案必定是在这个上下界限中的，需要实现的就是从这个歌上下界限中找出答案
- 我们每次取的 mid，其实就是我们每次在假设答案为 mid，二分的过程就是不断的推翻这个假设，然后再假设新的答案
- 需要满足的条件为：
	- 上面算法描述中的 sum 需要等于 k，这里的 k = (A.length + B.length) / 2. 如果 sum < k，很明显当前的 mid 偏小，需要增大，否则就说明当前的 mid 偏大，需要缩小.
- 最终在 start 与 end 相邻的时候退出循环，判断 start 跟 end 哪个符合条件即可得到最终结果
